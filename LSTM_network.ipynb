{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hainguyendangduc/DeepLearning_PJ/blob/main/LSTM_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nN1Qv5EZSo_",
        "outputId": "646ede27-fb7c-4dec-c062-e1c49499b4f1"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-26 10:08:02--  https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84188 (82K) [application/x-httpd-php]\n",
            "Saving to: ‘sentiment labelled sentences.zip.1’\n",
            "\n",
            "sentiment labelled  100%[===================>]  82.21K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-05-26 10:08:02 (559 KB/s) - ‘sentiment labelled sentences.zip.1’ saved [84188/84188]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqLDPXH6ZePB",
        "outputId": "1adf35e6-366b-4731-ecda-4b6b0055250d"
      },
      "source": [
        "!unzip sentences.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open sentences.zip, sentences.zip.zip or sentences.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLbDTzsQZnNC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "efe56336-b204-4c23-b6e2-5359c41b19c3"
      },
      "source": [
        "X = [] # input text\n",
        "y = [] # label\n",
        "\n",
        "f = open(file='a/amazon_cells_labelled.txt', mode='r')\n",
        "for line in f.readlines():\n",
        "  line = line.strip()\n",
        "  line = line.split(\"\\t\")\n",
        "  y.append(int(line[-1]))\n",
        "  X.append(line[0].strip())\n",
        "f.close()\n",
        "\n",
        "f = open('a/yelp_labelled.txt', 'r')\n",
        "for line in f.readlines():\n",
        "  line = line.strip()\n",
        "  line = line.split(\"\\t\")\n",
        "  y.append(int(line[-1]))\n",
        "  X.append(line[0].strip())\n",
        "f.close()\n",
        "\n",
        "f = open('a/imdb_labelled.txt', 'r')\n",
        "for line in f.readlines():\n",
        "  line = line.strip()\n",
        "  line = line.split(\"\\t\")\n",
        "  y.append(int(line[-1]))\n",
        "  X.append(line[0].strip())\n",
        "f.close()\n",
        "\n",
        "\n",
        "print(X[0])\n",
        "print(len(X))\n",
        "print(y[0])\n",
        "print(len(y))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-04488d99592c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a/amazon_cells_labelled.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'a/amazon_cells_labelled.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbH_GT3waBCW"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "print(X[0])\n",
        "print(len(X), len(y))\n",
        "plt.hist(y, bins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=0)"
      ],
      "metadata": {
        "id": "8RlFdoYSuvWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qywtvjWeyar"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = Tokenizer(num_words=2000)\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "X_seq_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_seq_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_seq_train[0])\n",
        "print(tokenizer.sequences_to_texts([[432, 47]]))\n",
        "print(X_train[0])"
      ],
      "metadata": {
        "id": "XoEcTcbLvPR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WahVTGjpfbVE"
      },
      "source": [
        "for i in X_seq_train[0]:\n",
        "   print(f\"{i} -----> {tokenizer.index_word[i]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_wXrdU-fl7d"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_length = max([len(t) for t in X_seq_train] + [len(t) for t in X_seq_test])\n",
        "print(max_length)\n",
        "X_pad_train = pad_sequences(sequences=X_seq_train, \n",
        "                              maxlen=max_length, \n",
        "                              padding='post')\n",
        "\n",
        "print(X_pad_train.shape)\n",
        "\n",
        "X_pad_test = pad_sequences(sequences=X_seq_test, \n",
        "                              maxlen=max_length,     \n",
        "                              padding='post')\n",
        "print(X_pad_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlJH9JLHldLf"
      },
      "source": [
        "X_pad_train[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPGe5IhDmWEX"
      },
      "source": [
        "# ML: X_pad ->  sentiment \n",
        "from tensorflow.keras.layers import Embedding, Input, Conv1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "vocab_size = 2000\n",
        "p = 0.1\n",
        "\n",
        "inp = Input(shape=(max_length))\n",
        "x = Embedding(vocab_size, 128, input_length=max_length)(inp)\n",
        "x = Dropout(p)(x)\n",
        "x = Conv1D(filters=32, kernel_size=3, \n",
        "           padding='same', activation='relu')(x)\n",
        "\n",
        "x = Dropout(p)(x)\n",
        "x = Conv1D(filters=16, kernel_size=3, \n",
        "           padding='same', activation='relu')(x)\n",
        "\n",
        "# collect features: poolling??? -> inovations \n",
        "# dãn ma trận 59 x 16 -> 944-vector\n",
        "#x = Flatten()(x)\n",
        "\n",
        "# Trung bình hoá các feature của chuỗi\n",
        "x = K.mean(x, keepdims=False, axis=1)\n",
        "\n",
        "# Fully connected layer for classification\n",
        "x = Dense(units=16, activation='relu')(x)\n",
        "x = Dropout(p)(x)\n",
        "x = Dense(units=1, activation='sigmoid')(x) # xác suất để x là positive \n",
        "\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.summary()\n",
        "\n",
        "# compile model -> optimizer, loss, evaluation metrics\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKVKmF92uiay"
      },
      "source": [
        "y = np.array(y)\n",
        "model.fit(X_pad_train, np.array(y_train), \n",
        "          epochs=10, batch_size=64, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_pad_test)\n",
        "y_pred[y_pred>0.5] = 1\n",
        "y_pred[y_pred<=0.5] = 0\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "FJaIiMGD1XT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLWFVQ7hvCjQ"
      },
      "source": [
        "def predict(seq, model):\n",
        "  test_text = [seq]\n",
        "  X_test = tokenizer.texts_to_sequences(test_text)\n",
        "  X_test_padd = pad_sequences(sequences=X_test, maxlen=max_length, padding='post')\n",
        "\n",
        "  p = model.predict(X_test_padd)[0, 0]\n",
        "\n",
        "  if p > 0.5: \n",
        "    return \"Positive\", p\n",
        "  else:\n",
        "    return \"Negatvie\", p\n",
        "\n",
        "seq = \"it is terrible today\"\n",
        "\n",
        "predict(seq, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdBVhToUvLhD"
      },
      "source": [
        "y[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JkJyTKMv3Yj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "seq = np.array([[0, 1, 0], \n",
        "                [1, 0,  0], \n",
        "                [0, 0, 1]])\n",
        "\n",
        "Wh = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) # 1 layer -> #row = #input, #col = #output\n",
        "Wx = np.array([[0.1, 0.2, 0.3], [.4, .5, .6], [.7, .8, .9]])\n",
        "\n",
        "ho = np.array([0, 0, 0])\n",
        "\n",
        "g = lambda z: np.tanh(z)\n",
        "for i in range(3):\n",
        "  h1 = g(np.dot(ho, Wh) + np.dot(seq[i], Wx))\n",
        "  print(h1)\n",
        "\n",
        "  ho = h1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7XTDZkS4pUt"
      },
      "source": [
        "# text -> RNN -> using final state as the features of text\n",
        "# seq = \"the phone was broken on shipping, terrible\" -> RNN -> final state ->FC layer -> sentiment\n",
        "\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "inp = Input(shape=(max_length))\n",
        "\n",
        "x = Embedding(vocab_size, 128, input_length=200)(inp)\n",
        "x, h, c = LSTM(units=128, return_state=True, return_sequences=True)(x)\n",
        "\n",
        "x = Conv1D(filters=32, kernel_size=3, \n",
        "           padding='same', activation='relu')(x)\n",
        "\n",
        "x = K.mean(x, keepdims=False, axis=1)\n",
        "y = Dense(units=64, activation='relu')(x)\n",
        "y = Dense(units=1, activation='sigmoid')(y)\n",
        "\n",
        "model = Model(inputs=inp, outputs=y)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# compile model -> optimizer, loss, evaluation metrics\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mGR9DNR7Kk3Q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmYP6j2x9KBX"
      },
      "source": [
        "model.fit(X_pad_train, np.array(y_train), \n",
        "          epochs=20, batch_size=64, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhlcCNaL9x_d"
      },
      "source": [
        "def predict(seq, model):\n",
        "  test_text = [seq]\n",
        "  X_test = tokenizer.texts_to_sequences(test_text)\n",
        "  X_test_padd = pad_sequences(sequences=X_test, maxlen=max_length, padding='post')\n",
        "\n",
        "  p = model.predict(X_test_padd)[0, 0]\n",
        "\n",
        "  if p > 0.5: \n",
        "    return \"Positive\", p\n",
        "  else:\n",
        "    return \"Negatvie\", p\n",
        "\n",
        "seq = \"the phone was broken on shipping\"\n",
        "\n",
        "predict(seq, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nblMN0Ep-EUe"
      },
      "source": [
        "y_pred = model.predict(X_pad_test)\n",
        "y_pred[y_pred>0.5] = 1\n",
        "y_pred[y_pred<=0.5] = 0\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8ieXurkxsReC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention\n",
        "# text -> RNN -> using final state as the features of text\n",
        "# seq = \"the phone was broken on shipping, terrible\" -> RNN -> final state ->FC layer -> sentiment\n",
        "\n",
        "from tensorflow.keras.layers import LSTM, Softmax\n",
        "\n",
        "\n",
        "inp = Input(shape=(max_length))\n",
        "\n",
        "x = Embedding(vocab_size, 128, input_length=200)(inp)\n",
        "x, h, c = LSTM(units=16, return_state=True, return_sequences=True)(x)\n",
        "x = Conv1D(filters=32, kernel_size=5, \n",
        "           padding='same', activation='relu')(x)\n",
        "\n",
        "# Attention part\n",
        "att = Conv1D(filters=1, kernel_size=1, \n",
        "           padding='same', activation='relu')(x)\n",
        "att = Softmax(axis=1)(att)\n",
        "\n",
        "x = K.sum(att * x, axis=1)\n",
        "y = Dense(units=64, activation='relu')(x)\n",
        "y = Dense(units=1, activation='sigmoid')(y)\n",
        "\n",
        "model = Model(inputs=inp, outputs=y)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# compile model -> optimizer, loss, evaluation metrics\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['acc'])"
      ],
      "metadata": {
        "id": "Ze2tFin5M9mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_pad_train, np.array(y_train), \n",
        "          epochs=20, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "spHgTxBHSaj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bidirectioal LSTM\n",
        "# text -> RNN -> using final state as the features of text\n",
        "# seq = \"the phone was broken on shipping, terrible\" -> RNN -> final state ->FC layer -> sentiment\n",
        "\n",
        "from tensorflow.keras.layers import LSTM, Softmax, Bidirectional\n",
        "\n",
        "\n",
        "inp = Input(shape=(max_length))\n",
        "\n",
        "x = Embedding(vocab_size, 16)(inp)\n",
        "x = Bidirectional(LSTM(units=16, return_sequences=True))(x)\n",
        "\n",
        "x = Conv1D(filters=32, kernel_size=5, \n",
        "           padding='same', activation='relu')(x)\n",
        "\n",
        "# Attention part\n",
        "att = Conv1D(filters=1, kernel_size=1, \n",
        "           padding='same', activation='relu')(x)\n",
        "att = Softmax(axis=1)(att)\n",
        "\n",
        "x = K.sum(att * x, axis=1)\n",
        "y = Dense(units=64, activation='relu')(x)\n",
        "y = Dense(units=1, activation='sigmoid')(y)\n",
        "\n",
        "model = Model(inputs=inp, outputs=y)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# compile model -> optimizer, loss, evaluation metrics\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['acc'])"
      ],
      "metadata": {
        "id": "g6Jq2nbtTEIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_pad_train, np.array(y_train), \n",
        "          epochs=20, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "nrSMEBgdX6JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7riIfvNEYirC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}